{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this Jupyter notebook we perform Sentiment Analysis for the Amazon Software dataset using Bag-of-Words. \n",
    "\n",
    "The Amazon Softare dataset is taken from this link: https://nijianmo.github.io/amazon/index.html \n",
    "\n",
    "We use BOW (Bag-of-Words) to perform Sentiment Analysis. \n",
    "\n",
    "We use two sources of data: \n",
    "\n",
    "(1) one zip file -i.e., \"Software.json.gz\"- contain the main dataset, and it contains the reviews of the clients;\n",
    "\n",
    "(2) another zip file -i.e., \"meta_Software.json.gz\"- contains the \"title\" of the product (i.e., the name of the product), and it contains the brand of the product and the main category of the product. \n",
    "\n",
    "The two datasets are merged together using the product ID which is included in both datasets. \n",
    "\n",
    "Description of the main variables in the main dataset (i.e., \"Software.json.gz\"): \n",
    "\n",
    "- reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "- asin - ID of the product, e.g. 0000013714\n",
    "- reviewerName - name of the reviewer\n",
    "- summary - summary of the review\n",
    "- reviewText - text of the review\n",
    "- overall - rating of the product\n",
    "- unixReviewTime - time of the review (unix time)\n",
    "- reviewTime - time of the review (raw)\n",
    "\n",
    "Description of the main variables in the metadata dataset (i.e., \"meta_Software.json.gz\"): \n",
    "\n",
    "- asin - ID of the product, e.g. 0000013714\n",
    "- title - name of the product\n",
    "- brand - brand name\n",
    "- main_cat - main category of the product (e.g., \"Software\"; \"All Electronics\")\n",
    "\n",
    "These sections are included in this Jupyter notebook: \n",
    "- In Section 1 we prepare the two datasets and we merge them using the product ID.  \n",
    "- In Section 2 we create the binary rating variable. The binary rating variable is used as the target of the BOW (bag-of-words) model in the following sections. \n",
    "- In Section 3 we apply pre-processing to the clients' reviews. We delete stopwords and numbers, and we use lower-case letters for all reviews; we remove punctuation, and we apply lemmatization. \n",
    "- In Section 4 we perform Sentiment Analysis. In Section 4 we use Grid search (i.e., GridSearchCV) in order to select the best (most predictive) model. We also use a pipeline within GridSearchCV. \n",
    "- In Section 5 we use grid search together with SMOTE (Synthetic Minority Over-sampling Technique); in this section we use the unbalanced dataset, and we correct unbalancedness through the use of SMOTE. \n",
    "- In Section 6 we draw a few conclusions about the effect of SMOTE on the \"precision\" and the \"recall\" of the estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 1: Preparing the Amazon (software) dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Array\n",
    "import numpy as np\n",
    "\n",
    "# Decompress the file\n",
    "import gzip\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline\n",
    "\n",
    "# Datetime\n",
    "#from datetime import datetime\n",
    "\n",
    "## Warnings\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "        \n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the main dataset which includes the reviews and the ratings: The main dataset is included in 2 ZIP files: \"Software_df1.json.gz\" and \"Software_df2.json.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(185001, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>A1YCCU0YRLS0FE</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "      <td>1393113600</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0      4.0      True  03 11, 2014  A240ORQ2LF9LUI  0077613252   \n",
       "1      4.0      True  02 23, 2014  A1YCCU0YRLS0FE  0077613252   \n",
       "\n",
       "                        style         reviewerName  \\\n",
       "0  {'Format:': ' Loose Leaf'}           Michelle W   \n",
       "1  {'Format:': ' Loose Leaf'}  Rosalind White Ames   \n",
       "\n",
       "                                          reviewText         summary  \\\n",
       "0  The materials arrived early and were in excell...  Material Great   \n",
       "1  I am really enjoying this book with the worksh...          Health   \n",
       "\n",
       "   unixReviewTime  vote image  \n",
       "0      1394496000  None  None  \n",
       "1      1393113600  None  None  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df1 = getDF('data/Software_df1.json.gz')\n",
    "\n",
    "print(review_df1.shape)\n",
    "review_df1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(421433, 12)\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### Drop duplicated records\n",
    "###########################\n",
    "review_df = review_df.drop_duplicates(subset='reviewText', keep='first')\n",
    "print(review_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### Rename column \"overall\" to \"Rating\" \n",
    "###########################\n",
    "review_df = review_df.rename(columns={'overall':'Rating'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### Save it as CSV file\n",
    "###########################\n",
    "review_df.to_csv('data/review_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the metadata to extract the title, brand and main category of the product. The \"title\" is the name of the product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26790, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>tech1</th>\n",
       "      <th>description</th>\n",
       "      <th>fit</th>\n",
       "      <th>title</th>\n",
       "      <th>also_buy</th>\n",
       "      <th>image</th>\n",
       "      <th>tech2</th>\n",
       "      <th>brand</th>\n",
       "      <th>feature</th>\n",
       "      <th>rank</th>\n",
       "      <th>also_view</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>similar_item</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>asin</th>\n",
       "      <th>details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>HOLT PHYSICS LESSON PRESENTATION CD-ROM QUICK ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>HOLT. RINEHART AND WINSTON</td>\n",
       "      <td>[]</td>\n",
       "      <td>25,550 in Software (</td>\n",
       "      <td>[]</td>\n",
       "      <td>Software</td>\n",
       "      <td></td>\n",
       "      <td>&lt;/div&gt;</td>\n",
       "      <td>.a-box-inner{background-color:#fff}#alohaBuyBo...</td>\n",
       "      <td>0030672120</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[, &lt;b&gt;Latin rhythms that will get your kids si...</td>\n",
       "      <td></td>\n",
       "      <td>Sing, Watch, &amp;amp; Learn Spanish (DVD + Guide)...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td></td>\n",
       "      <td>McGraw Hill</td>\n",
       "      <td>[]</td>\n",
       "      <td>15,792 in Software (</td>\n",
       "      <td>[]</td>\n",
       "      <td>Software</td>\n",
       "      <td></td>\n",
       "      <td>&lt;/div&gt;</td>\n",
       "      <td></td>\n",
       "      <td>0071480935</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category tech1                                        description fit  \\\n",
       "0       []                                                       []       \n",
       "1       []        [, <b>Latin rhythms that will get your kids si...       \n",
       "\n",
       "                                               title also_buy  \\\n",
       "0  HOLT PHYSICS LESSON PRESENTATION CD-ROM QUICK ...       []   \n",
       "1  Sing, Watch, &amp; Learn Spanish (DVD + Guide)...       []   \n",
       "\n",
       "                                               image tech2  \\\n",
       "0                                                 []         \n",
       "1  [https://images-na.ssl-images-amazon.com/image...         \n",
       "\n",
       "                        brand feature                  rank also_view  \\\n",
       "0  HOLT. RINEHART AND WINSTON      []  25,550 in Software (        []   \n",
       "1                 McGraw Hill      []  15,792 in Software (        []   \n",
       "\n",
       "   main_cat similar_item    date  \\\n",
       "0  Software               </div>   \n",
       "1  Software               </div>   \n",
       "\n",
       "                                               price        asin details  \n",
       "0  .a-box-inner{background-color:#fff}#alohaBuyBo...  0030672120     NaN  \n",
       "1                                                     0071480935     NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = getDF('data/meta_Software.json.gz')\n",
    "print(metadata.shape)\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21110, 18)\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### Drop duplicated records\n",
    "###########################\n",
    "metadata = metadata.drop_duplicates(subset='title', keep='first')\n",
    "print(metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### Keep only 'title', 'brand' and 'asin' (i.e., the product ID) for the metadata dataframe \n",
    "###########################\n",
    "metadata = metadata[['title', 'brand', 'asin', 'main_cat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### Save it as CSV file\n",
    "###########################\n",
    "metadata.to_csv('data/metadata.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating                 0\n",
      "verified               0\n",
      "reviewTime             0\n",
      "reviewerID             0\n",
      "asin                   0\n",
      "style             206584\n",
      "reviewerName          21\n",
      "reviewText             1\n",
      "summary               40\n",
      "unixReviewTime         0\n",
      "vote              298680\n",
      "image             419964\n",
      "dtype: int64\n",
      "\n",
      "title       0\n",
      "brand       0\n",
      "asin        0\n",
      "main_cat    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "####  Data cleaning: check missing values \n",
    "#############################\n",
    "print(review_df.isna().sum())\n",
    "print('')\n",
    "print(metadata.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating                 0\n",
       "verified               0\n",
       "reviewTime             0\n",
       "reviewerID             0\n",
       "asin                   0\n",
       "style             206565\n",
       "reviewerName          21\n",
       "reviewText             0\n",
       "summary                0\n",
       "unixReviewTime         0\n",
       "vote              298647\n",
       "image             419923\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################\n",
    "### Drop rows with missing reviewText and summary\n",
    "################################\n",
    "review_df = review_df.dropna(subset=['reviewText', 'summary'])\n",
    "review_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We merge the two dataset using the product ID (i.e., \"asin\").**\n",
    "\n",
    "**Some of the products (i.e., \"asin\") that are included in the main dataset are not included in the metadata.** \n",
    "\n",
    "**Thus, we use inner join on \"asin\" because we want to have all information (e.g., information on \"title\", \"brand\", \"main category\") for all the reviews included in the final dataset.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = review_df.merge(metadata, how='inner', on='asin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410492, 15)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating                 0\n",
       "verified               0\n",
       "reviewTime             0\n",
       "reviewerID             0\n",
       "asin                   0\n",
       "style             203413\n",
       "reviewerName          21\n",
       "reviewText             0\n",
       "summary                0\n",
       "unixReviewTime         0\n",
       "vote              290118\n",
       "image             409064\n",
       "title                  0\n",
       "brand                  0\n",
       "main_cat               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### We drop columns 'image', 'vote', 'style', 'verified' as these columns are redundant and useless.  \n",
    "################################\n",
    "review_df.drop('image', inplace=True, axis=1)\n",
    "review_df.drop('vote', inplace=True, axis=1)\n",
    "review_df.drop('style', inplace=True, axis=1)\n",
    "review_df.drop('verified', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(410492, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>main_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>A1YCCU0YRLS0FE</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "      <td>1393113600</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating   reviewTime      reviewerID        asin         reviewerName  \\\n",
       "0     4.0  03 11, 2014  A240ORQ2LF9LUI  0077613252           Michelle W   \n",
       "1     4.0  02 23, 2014  A1YCCU0YRLS0FE  0077613252  Rosalind White Ames   \n",
       "\n",
       "                                          reviewText         summary  \\\n",
       "0  The materials arrived early and were in excell...  Material Great   \n",
       "1  I am really enjoying this book with the worksh...          Health   \n",
       "\n",
       "   unixReviewTime                                              title  \\\n",
       "0      1394496000  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "1      1393113600  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "\n",
       "                                              brand  main_cat  \n",
       "0  McGraw-Hill Humanities/Social Sciences/Languages  Software  \n",
       "1  McGraw-Hill Humanities/Social Sciences/Languages  Software  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(review_df.shape)\n",
    "review_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are lots of categories (i.e., main_cat). Since we want to focus only on software, we discard observations for products that do not belong to the category of software** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Software', 'Books', 'Video Games', 'Movies &amp; TV',\n",
       "       'Cell Phones &amp; Accessories', 'Office Products',\n",
       "       'Toys &amp; Games', 'All Electronics', 'Cell Phones & Accessories',\n",
       "       'GPS & Navigation', 'Movies & TV', 'Toys & Games',\n",
       "       'Musical Instruments', 'GPS &amp; Navigation',\n",
       "       'Arts, Crafts &amp; Sewing', 'Home Audio &amp; Theater',\n",
       "       'Camera & Photo', 'Car Electronics', 'Arts, Crafts & Sewing',\n",
       "       'Home Audio & Theater', 'Computers', 'Tools & Home Improvement',\n",
       "       'Amazon Home', 'Camera &amp; Photo', 'Baby', 'Pet Supplies',\n",
       "       'Tools &amp; Home Improvement',\n",
       "       '<img src=\"https://images-na.ssl-images-amazon.com/images/G/01/digital/music/logos/amzn_music_logo_subnav._CB471835632_.png\" class=\"nav-categ-image\" alt=\"Digital Music\"/>',\n",
       "       'Automotive', 'Health & Personal Care', 'Sports &amp; Outdoors',\n",
       "       'Health &amp; Personal Care', 'Sports & Outdoors',\n",
       "       '<img src=\"https://m.media-amazon.com/images/G/01/digital/music/logos/amzn_music_logo_subnav._CB471835632_.png\" class=\"nav-categ-image\" alt=\"Digital Music\"/>',\n",
       "       'Audible Audiobooks', 'Industrial & Scientific',\n",
       "       'Industrial &amp; Scientific', '',\n",
       "       '<img src=\"https://images-na.ssl-images-amazon.com/images/G/01/nav2/images/gui/amazon-fashion-store-new._CB520838675_.png\" class=\"nav-categ-image\" alt=\"AMAZON FASHION\"/>',\n",
       "       'All Beauty', 'Grocery', 'Portable Audio & Accessories',\n",
       "       '<img src=\"https://images-na.ssl-images-amazon.com/images/G/01/digital/music/logos/amzn_music_logo_subnav._CB471835632_.png\" class=\"nav-categ-image\" alt=\"Digital Music\" />'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.main_cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = review_df[review_df.main_cat == 'Software']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(389927, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>main_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>A1YCCU0YRLS0FE</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "      <td>1393113600</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating   reviewTime      reviewerID        asin         reviewerName  \\\n",
       "0     4.0  03 11, 2014  A240ORQ2LF9LUI  0077613252           Michelle W   \n",
       "1     4.0  02 23, 2014  A1YCCU0YRLS0FE  0077613252  Rosalind White Ames   \n",
       "\n",
       "                                          reviewText         summary  \\\n",
       "0  The materials arrived early and were in excell...  Material Great   \n",
       "1  I am really enjoying this book with the worksh...          Health   \n",
       "\n",
       "   unixReviewTime                                              title  \\\n",
       "0      1394496000  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "1      1393113600  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "\n",
       "                                              brand  main_cat  \n",
       "0  McGraw-Hill Humanities/Social Sciences/Languages  Software  \n",
       "1  McGraw-Hill Humanities/Social Sciences/Languages  Software  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(review_df.shape)\n",
    "review_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We create the client's review text which is the ensemble of 'summary' and 'reviewText': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>review_2.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "      <td>Material Great The materials arrived early and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating   reviewTime      reviewerID        asin reviewerName  \\\n",
       "0     4.0  03 11, 2014  A240ORQ2LF9LUI  0077613252   Michelle W   \n",
       "\n",
       "                                          reviewText         summary  \\\n",
       "0  The materials arrived early and were in excell...  Material Great   \n",
       "\n",
       "   unixReviewTime                                              title  \\\n",
       "0      1394496000  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "\n",
       "                                              brand  main_cat  \\\n",
       "0  McGraw-Hill Humanities/Social Sciences/Languages  Software   \n",
       "\n",
       "                                          review_2.0  \n",
       "0  Material Great The materials arrived early and...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['review_2.0'] = review_df['summary'] + \" \" + review_df['reviewText'] \n",
    "review_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# *Section 2: we create the binary rating variable. The binary rating variable is used as the target in the following sections.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    171743\n",
       "1.0     93061\n",
       "4.0     62584\n",
       "3.0     34306\n",
       "2.0     28233\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use rating = {1,2,3} as bad/neutral rating, and we use rating = {4,5} as good rating. \n",
    "See the discussion at https://sellercentral.amazon.com/forums/t/does-a-neutral-3-star-rating-on-your-feedback-count-against-odr/1081/14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>review_2.0</th>\n",
       "      <th>Rating_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "      <td>Material Great The materials arrived early and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating   reviewTime      reviewerID        asin reviewerName  \\\n",
       "0     4.0  03 11, 2014  A240ORQ2LF9LUI  0077613252   Michelle W   \n",
       "\n",
       "                                          reviewText         summary  \\\n",
       "0  The materials arrived early and were in excell...  Material Great   \n",
       "\n",
       "   unixReviewTime                                              title  \\\n",
       "0      1394496000  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "\n",
       "                                              brand  main_cat  \\\n",
       "0  McGraw-Hill Humanities/Social Sciences/Languages  Software   \n",
       "\n",
       "                                          review_2.0  Rating_binary  \n",
       "0  Material Great The materials arrived early and...              1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['Rating_binary'] = review_df['Rating'].apply(lambda x: 0 if x < 4 else 1)\n",
    "review_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    234327\n",
       "0    155600\n",
       "Name: Rating_binary, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['Rating_binary'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 3: Apply pre-processing to the clients' reviews* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower-case: \n",
    "#df['review_2.0'] = df['review_2.0'].str.replace(r'\\n', ' ')\n",
    "def remove_symbols(mystring):\n",
    "    new_string = mystring.replace(r'\\n', ' ')\n",
    "    return new_string \n",
    "\n",
    "\n",
    "# remove numbers: \n",
    "def remove_nmbrs(mystring):\n",
    "    mystring_no_numbers = ''.join(word for word in mystring if not word.isdigit())\n",
    "    return mystring_no_numbers \n",
    "\n",
    "\n",
    "#df['review_2.0'] = df['review_2.0'].astype(str)\n",
    "def stringify(mystring):\n",
    "    new_string = str(mystring) \n",
    "    return new_string \n",
    "\n",
    "\n",
    "\n",
    "# Remove Punctuation:\n",
    "def punct(mystring):\n",
    "    import string \n",
    "    list_s_p = string.punctuation\n",
    "    for punctuation in list_s_p:\n",
    "        mystring = mystring.replace(punctuation, ' ')\n",
    "    return mystring\n",
    "\n",
    "\n",
    "# lower-case: \n",
    "def lower_it(mystring):\n",
    "    lowered_mystring = mystring.lower()\n",
    "    return lowered_mystring \n",
    "\n",
    "\n",
    "\n",
    "#Remove StopWords:\n",
    "def stopwords(mystring):\n",
    "    from nltk.corpus import stopwords \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(mystring) \n",
    "    splitting_string = [w for w in word_tokens if not w in stop_words] #creates a list!\n",
    "    text  = ' '.join(word for word in splitting_string)\n",
    "    return text \n",
    " \n",
    "\n",
    "#Lemmatize:\n",
    "def lem(mystring):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    splitting_text = mystring.split()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in splitting_text]\n",
    "    mystring = ' '.join(word for word in lemmatized)\n",
    "    return mystring\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = remove_symbols(text)\n",
    "    text = remove_nmbrs(text)\n",
    "    text = stringify(text)\n",
    "\n",
    "    text = punct(text)\n",
    "    text = lower_it(text)\n",
    "    text = stopwords(text)\n",
    "    text = lem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df['review_2.0'] =  review_df['review_2.0'].map(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>review_2.0</th>\n",
       "      <th>Rating_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "      <td>material great material arrived early excellen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>A1YCCU0YRLS0FE</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "      <td>1393113600</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "      <td>health really enjoying book worksheet make rev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>02 17, 2014</td>\n",
       "      <td>A1BJHRQDYVAY2J</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Allan R. Baker</td>\n",
       "      <td>IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...</td>\n",
       "      <td>ARE YOU KIDING ME?</td>\n",
       "      <td>1392595200</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "      <td>kiding taking class waste money called book bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating   reviewTime      reviewerID        asin         reviewerName  \\\n",
       "0     4.0  03 11, 2014  A240ORQ2LF9LUI  0077613252           Michelle W   \n",
       "1     4.0  02 23, 2014  A1YCCU0YRLS0FE  0077613252  Rosalind White Ames   \n",
       "2     1.0  02 17, 2014  A1BJHRQDYVAY2J  0077613252       Allan R. Baker   \n",
       "\n",
       "                                          reviewText             summary  \\\n",
       "0  The materials arrived early and were in excell...      Material Great   \n",
       "1  I am really enjoying this book with the worksh...              Health   \n",
       "2  IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...  ARE YOU KIDING ME?   \n",
       "\n",
       "   unixReviewTime                                              title  \\\n",
       "0      1394496000  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "1      1393113600  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "2      1392595200  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "\n",
       "                                              brand  main_cat  \\\n",
       "0  McGraw-Hill Humanities/Social Sciences/Languages  Software   \n",
       "1  McGraw-Hill Humanities/Social Sciences/Languages  Software   \n",
       "2  McGraw-Hill Humanities/Social Sciences/Languages  Software   \n",
       "\n",
       "                                          review_2.0  Rating_binary  \n",
       "0  material great material arrived early excellen...              1  \n",
       "1  health really enjoying book worksheet make rev...              1  \n",
       "2  kiding taking class waste money called book bo...              0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 4: Grid Search using a pipeline which is composed by (1) TfidfVectorizer; (2) Algorithm for classification.*\n",
    "# *In this section we use a balanced dataset: I.e., the number of 0s is the same as the number of 1s.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(389927, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    234327\n",
       "0    155600\n",
       "Name: Rating_binary, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df_balanced = review_df.copy()\n",
    "print(review_df_balanced.shape)\n",
    "review_df_balanced['Rating_binary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155600\n",
      "1    155600\n",
      "0    155600\n",
      "Name: Rating_binary, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "#### Balancing the dataframe: \n",
    "################################\n",
    "\n",
    "num = (review_df_balanced['Rating_binary'].value_counts()).min()\n",
    "print(num )\n",
    "#155600\n",
    "\n",
    "df_pos = review_df_balanced[review_df_balanced['Rating_binary'] == 1].sample(num, random_state=0)\n",
    "df_neg = review_df_balanced[review_df_balanced['Rating_binary'] == 0].sample(num)\n",
    "review_df_balanced = pd.concat([df_pos, df_neg], verify_integrity=True)\n",
    "print(review_df_balanced['Rating_binary'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Baseline Estimator (sklearn.dummy.DummyClassifier)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "{}\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from imblearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000) \n",
    "dummy = DummyClassifier()  \n",
    "\n",
    "pipeline_dummy = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('dummy', dummy)])\n",
    "parameters = {}\n",
    "# Perform simply CV (Cross-validation) without searching the best parameters:\n",
    "grid_search = GridSearchCV(pipeline_dummy, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_balanced['review_2.0']\n",
    "y = review_df_balanced['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "#0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The accuracy of the baseline estimator is 0.5: I.e., only 50% of the target is correctly predicted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes with GridSearchCV for the \"alpha\" parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "{'nb__alpha': 1}\n",
      "0.8554016709511568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from imblearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000) \n",
    "naive_bayes = MultinomialNB()  \n",
    "\n",
    "pipeline_dummy = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('nb', naive_bayes)])\n",
    "\n",
    "parameters = {'nb__alpha': (0, 0.01, 0.05, 0.1, 0.3, 0.5, 1),\n",
    "              }\n",
    "# Perform simply CV (Cross-validation) without searching the best parameters:\n",
    "grid_search = GridSearchCV(pipeline_dummy, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_balanced['review_2.0']\n",
    "y = review_df_balanced['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "#Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
    "#{'nb__alpha': 1}\n",
    "#0.8554016709511568\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best accuracy of the Naive Bayes estimator is 0.855: I.e., 85.5% of the target is correctly predicted. This is a fairly large improvement with respect to the baseline estimator.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with GridSearchCV for the \"penalty\" parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "{'log__penalty': 'l2'}\n",
      "0.9003566838046272\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from imblearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000) \n",
    "log_reg = LogisticRegression()  \n",
    "\n",
    "pipeline_dummy = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('log', log_reg)])\n",
    "\n",
    "parameters = {'log__penalty': ['l1','l2']\n",
    "              }\n",
    "# Perform simply CV (Cross-validation) without searching the best parameters:\n",
    "grid_search = GridSearchCV(pipeline_dummy, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_balanced['review_2.0']\n",
    "y = review_df_balanced['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "#Fitting 5 folds for each of 2 candidates, totalling 10 fits \n",
    "#0.89\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best accuracy of the Logistic Regression is around 0.90: I.e., aorund 90% of the target is correctly predicted. This is a fairly large improvement with respect to the baseline estimator.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree algorithm with GridSearchCV for the \"max_depth\" parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "{'tree__max_depth': 8}\n",
      "0.7578374035989717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from imblearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000) \n",
    "tree = DecisionTreeClassifier()  \n",
    "\n",
    "pipeline_dummy = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('tree', tree)])\n",
    "\n",
    "parameters = {'tree__max_depth': [2, 3, 6, 8]\n",
    "              }\n",
    "\n",
    "# Perform simply CV (Cross-validation) without searching the best parameters:\n",
    "grid_search = GridSearchCV(pipeline_dummy, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_balanced['review_2.0']\n",
    "y = review_df_balanced['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "#Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
    "#{'tree__max_depth': 8}\n",
    "#0.7578374035989717\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The performance of the decision tree is worse than Logistic Regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost classifier with GridSearchCV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[14:01:35] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "{'xgboost__max_depth': 7}\n",
      "0.7877988431876606\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier     \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from imblearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000) \n",
    "xgboost = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='binary:logistic',\n",
    "                    silent=True, nthread=-1)\n",
    "\n",
    "pipeline_dummy = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('xgboost', xgboost)])\n",
    "\n",
    "parameters = {'xgboost__max_depth': [6,7]\n",
    "              }\n",
    "\n",
    "# Perform simply CV (Cross-validation) without searching the best parameters:\n",
    "grid_search = GridSearchCV(pipeline_dummy, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_balanced['review_2.0']\n",
    "y = review_df_balanced['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "#Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
    "#{'xgboost__max_depth': 7}\n",
    "#0.7877988431876606"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The performance of XGBoost classifier is worse than Logistic Regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion: Logistic Regression is the best algorithm for our task. Moreover, the Lasso penalty -i.e., l2-penalty- is the best penalty for Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thus, we save the Logistic Regression model with Lasso penalty as our favorite estimator. We use joblib to save the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "{'log__penalty': 'l2'}\n",
      "0.8909254498714653\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from imblearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000) \n",
    "log_reg = LogisticRegression()  \n",
    "\n",
    "pipeline_dummy = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('log', log_reg)])\n",
    "\n",
    "parameters = {'log__penalty': ['l1','l2']\n",
    "              }\n",
    "# Perform simply CV (Cross-validation) without searching the best parameters:\n",
    "grid_search = GridSearchCV(pipeline_dummy, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_balanced['review_2.0']\n",
    "y = review_df_balanced['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "#Fitting 5 folds for each of 2 candidates, totalling 10 fits \n",
    "#0.89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finalized_model.joblib']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.joblib'\n",
    "#joblib.dump(grid_search.best_estimator_, filename, compress = 1)\n",
    "joblib.dump(grid_search.best_estimator_, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-sample prediction: We use 2 reviews from Amazon UK (Software) to further test our model (The above model was estimated with data from Amazon.com): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of review with bad rating:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = joblib.load(filename)\n",
    "\n",
    "#I use one example of a review from Amazon UK (Software): https://www.amazon.co.uk/Microsoft-Professional-Genuine-Lifetime-Product/dp/B00WYPDA4C/ref=sr_1_16?dchild=1&keywords=microsoft+office&qid=1608730193&s=software&sr=1-16\n",
    "#This review has a bad rating (i.e., 2 stars):\n",
    "example_of_review = 'When I received this product, I thought I would get the actual disk from Microsoft. However, it was a burned disk with instructions on how to download it and activate it. The instructions were very confusing and when I put in my product key, it said it was not a valid key. Got on the phone and could get no help, so had to send it back and buy from someone else. Very happy with the new product which was office 2010. If you buy this hope you have better luck than me.'\n",
    "\n",
    "#Apply the preprocessing: \n",
    "example_of_review =  preprocessing(example_of_review)\n",
    "\n",
    "#Transform the \"review\" into a list (or an iterable) containing a single element: \n",
    "final_review = [example_of_review]\n",
    "result = loaded_model.predict(final_review)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of review with good rating:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = joblib.load(filename)\n",
    "\n",
    "#I use one example of a review from Amazon UK (Software): https://www.amazon.co.uk/Microsoft-Professional-Genuine-Lifetime-Product/dp/B00WYPDA4C/ref=sr_1_16?dchild=1&keywords=microsoft+office&qid=1608730193&s=software&sr=1-16\n",
    "#This review has a good rating (i.e., 5 stars):\n",
    "example_of_review = 'I don\\'t usually write reviews but this compnay deserves a big shout our for great customer service. I am not computer savey and was really messing up installation, so i called the tech support and ended up talking to Mitchell he was patient and didn\\'t make me feel like and idoit for messing things up. If i hadn\\'t been for him this review would have been much different. He went out of his way to make sure eveything was installed right and even made called microsoft because i had messed things up so bad. Thank you thank you for companys with intergrity and great customer service.'\n",
    "\n",
    "#Apply the preprocessing: \n",
    "example_of_review =  preprocessing(example_of_review)\n",
    "\n",
    "#Transform the \"review\" into a list (or an iterable) containing a single element: \n",
    "final_review = [example_of_review]\n",
    "result = loaded_model.predict(final_review)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion for out-of-sample prediction: both cases are correctly predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 5: Grid Search using a pipeline which is composed by (1) TfidfVectorizer; (2) SMOTE (Synthetic Minority Over-sampling Technique); (3) Logistic regression. In this section we use the unbalanced dataset, and we do not balance the dataset because we use SMOTE.*\n",
    "\n",
    "# We use only 10% of the original (unbalanced) dataset, because of memory issues which happened during the estimation of the full (unbalanced) dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Below, we use the best model that was found in Section 4, but we additionally use SMOTE on the unbalanced dataset.**\n",
    "\n",
    "In order to use SMOTE, we need to: \n",
    "- use \"from imblearn.pipeline import Pipeline\";\n",
    "- import SMOTE from  imblearn.over_sampling; \n",
    "- use the unbalanced dataset which contains 10% of the original observations (i.e., review_df_restricted). See below for more details about review_df_restricted. \n",
    "\n",
    "All other settings are exactly the same as for our preferred model from Section 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(389927, 13)\n",
      "(38992, 13)\n"
     ]
    }
   ],
   "source": [
    "print(review_df.shape)\n",
    "#(389927, 13)\n",
    "\n",
    "import math \n",
    "n_obs = int(math.floor(389927/10))\n",
    "review_df_restricted  = review_df.sample(n_obs, random_state=1)\n",
    "print(review_df_restricted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    23426\n",
       "0    15566\n",
       "Name: Rating_binary, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pre-processing was alread applid above for the full dataset. \n",
    "#review_df_restricted['review_2.0'] =  review_df_restricted['review_2.0'].map(preprocessing)\n",
    "\n",
    "review_df_restricted['Rating_binary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "{'log_reg__penalty': 'l2'}\n",
      "0.8764619239989765\n"
     ]
    }
   ],
   "source": [
    "# testing the model with gridsearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000)\n",
    "log_reg = LogisticRegression() \n",
    "\n",
    "pipeline_log_reg = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('sampling', SMOTE()),\n",
    "        ('log_reg', log_reg)])\n",
    "\n",
    "parameters = {'log_reg__penalty': ['l2']} \n",
    "\n",
    "# Perform cross validation: \n",
    "grid_search = GridSearchCV(pipeline_log_reg, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_restricted['review_2.0']\n",
    "y = review_df_restricted['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We also save our model which is based on SMOTE in a joblib file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finalized_model_SMOTE.joblib']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# save the model to disk\n",
    "filename = 'finalized_model_SMOTE.joblib'\n",
    "#joblib.dump(grid_search.best_estimator_, filename, compress = 1)\n",
    "joblib.dump(grid_search.best_estimator_, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-sample prediction: We use 2 reviews from Amazon UK (Software) to further test our model (The above model was estimated with data from Amazon.com): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of review with bad rating:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = joblib.load('finalized_model_SMOTE.joblib')\n",
    "\n",
    "#I use one example of a review from Amazon UK (Software): https://www.amazon.co.uk/Microsoft-Professional-Genuine-Lifetime-Product/dp/B00WYPDA4C/ref=sr_1_16?dchild=1&keywords=microsoft+office&qid=1608730193&s=software&sr=1-16\n",
    "#This review has a bad rating (i.e., 2 stars):\n",
    "example_of_review = 'When I received this product, I thought I would get the actual disk from Microsoft. However, it was a burned disk with instructions on how to download it and activate it. The instructions were very confusing and when I put in my product key, it said it was not a valid key. Got on the phone and could get no help, so had to send it back and buy from someone else. Very happy with the new product which was office 2010. If you buy this hope you have better luck than me.'\n",
    "\n",
    "#Apply the preprocessing: \n",
    "example_of_review =  preprocessing(example_of_review)\n",
    "\n",
    "#Transform the \"review\" into a list (or an iterable) containing a single element: \n",
    "final_review = [example_of_review]\n",
    "result = loaded_model.predict(final_review)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of review with good rating:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = joblib.load('finalized_model_SMOTE.joblib')\n",
    "\n",
    "#I use one example of a review from Amazon UK (Software): https://www.amazon.co.uk/Microsoft-Professional-Genuine-Lifetime-Product/dp/B00WYPDA4C/ref=sr_1_16?dchild=1&keywords=microsoft+office&qid=1608730193&s=software&sr=1-16\n",
    "#This review has a good rating (i.e., 5 stars):\n",
    "example_of_review = 'I don\\'t usually write reviews but this compnay deserves a big shout our for great customer service. I am not computer savey and was really messing up installation, so i called the tech support and ended up talking to Mitchell he was patient and didn\\'t make me feel like and idoit for messing things up. If i hadn\\'t been for him this review would have been much different. He went out of his way to make sure eveything was installed right and even made called microsoft because i had messed things up so bad. Thank you thank you for companys with intergrity and great customer service.'\n",
    "\n",
    "#Apply the preprocessing: \n",
    "example_of_review =  preprocessing(example_of_review)\n",
    "\n",
    "#Transform the \"review\" into a list (or an iterable) containing a single element: \n",
    "final_review = [example_of_review]\n",
    "result = loaded_model.predict(final_review)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion for out-of-sample prediction: both cases are correctly predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 6 we draw a few conclusions about the effect of SMOTE on the \"precision\" and the \"recall\" of the estimator.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6.1) SMOTE is used to \"balance\" the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We use \"precision\" as the scoring measure  (and do not use accuracy), and we estimate the model using SMOTE.**\n",
    "\n",
    "**We also show the confusion matrix using SMOTE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "{'log_reg__penalty': 'l2'}\n",
      "0.9107987821384562\n"
     ]
    }
   ],
   "source": [
    "# testing the model with gridsearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000)\n",
    "log_reg = LogisticRegression() \n",
    "\n",
    "pipeline_log_reg = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('sampling', SMOTE()),\n",
    "        ('log_reg', log_reg)])\n",
    "\n",
    "parameters = {'log_reg__penalty': ['l2']} \n",
    "\n",
    "# Perform cross validation: \n",
    "grid_search = GridSearchCV(pipeline_log_reg, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"precision\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_restricted['review_2.0']\n",
    "y = review_df_restricted['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "#Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
    "\n",
    "# precision with SMOTE: \n",
    "#{'log_reg__penalty': 'l2'}\n",
    "#0.9107987821384562"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14128,  1438],\n",
       "       [ 2170, 21256]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = grid_search.best_estimator_.predict(X)\n",
    "confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion: the \"precision\" is 0.910 using SMOTE AND THE CONFUCSION MATRIX IS: \n",
    "\n",
    "# [14128,  1438],\n",
    "\n",
    "# [ 2170, 21256]\n",
    "\n",
    "# where TN = 14128; TP = 21256; FP (false positives) = 1438; FN (false negatives) = 2170. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 6.2) SMOTE is OMITTED below.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We use \"precision\" as the scoring measure  (and do not use accuracy), and we estimate the model WITHOUT SMOTE.**\n",
    "\n",
    "**We also show the confusion matrix WITHOUT SMOTE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "{'log_reg__penalty': 'l2'}\n",
      "0.8900174434732577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[13517,  2049],\n",
       "       [ 1516, 21910]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the model with gridsearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000)\n",
    "log_reg = LogisticRegression() \n",
    "\n",
    "pipeline_log_reg = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        #('sampling', SMOTE()),\n",
    "        ('log_reg', log_reg)])\n",
    "\n",
    "parameters = {'log_reg__penalty': ['l2']} \n",
    "\n",
    "# Perform cross validation: \n",
    "grid_WITHOUT_SMOTE = GridSearchCV(pipeline_log_reg, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"precision\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_restricted['review_2.0']\n",
    "y = review_df_restricted['Rating_binary']\n",
    "\n",
    "grid_WITHOUT_SMOTE.fit(X,y)\n",
    "grid_WITHOUT_SMOTE.best_params_\n",
    "grid_WITHOUT_SMOTE.best_score_\n",
    "print(grid_WITHOUT_SMOTE.best_params_)\n",
    "print(grid_WITHOUT_SMOTE.best_score_)\n",
    "#Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
    "#{'log_reg__penalty': 'l2'}\n",
    "#0.8900174434732577\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13517,  2049],\n",
       "       [ 1516, 21910]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_WITHOUT_SMOTE = grid_WITHOUT_SMOTE.best_estimator_.predict(X)\n",
    "confusion_matrix(y, y_pred_WITHOUT_SMOTE)\n",
    "\n",
    "#array([[13517,  2049],\n",
    "#       [ 1516, 21910]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion: the \"precision\" is 0.890 when we omit SMOTE from our pipeline and the confusion matrix is:  \n",
    "\n",
    "# [13517,  2049],\n",
    "\n",
    "# [ 1516, 21910]\n",
    "\n",
    "# where TN = 14128; TP = 21256; FP (false positives) = 1438; FN (false negatives) = 2170. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 6.3) Comparison of the estimator with and without SMOTE.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The precision is higher in the case that we use SMOTE (compared to the case that we omit SMOTE).**\n",
    "\n",
    "# Explanation: \n",
    "- The dataset (review_df_restricted) contains many more 1s than 0s (zeros).\n",
    "- SMOTE forces the estimator to see more 0s (zeros) as the target variable (compared to the case that we omit SMOTE from the pipeline). \n",
    "- As a result of the above bullet point, we have that the number of FP (false positives) decreases drastically for the model that includes SMOTE (compared to the case that we omit SMOTE from the pipeline). This is exactly what we would expect from the use of SMOTE in our case!\n",
    "- Precision's formula: TP / (TP + FP). \n",
    "- In conclusion, SMOTE forces the estimator to see more 0s (zeros), and this implies that the number of FP (false positives) decreases by almost 25% compared to the the number of FP for the non-SMOTEd model. Finally, the fact that the number of FP is much lower for the SMOTEd model implies that the \"Precision\" is higher for the SMOTEd model. This is also as expected for our specific dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final remark on the tradeoff between Recall and Precision:\n",
    "\n",
    "**On the other hand, the recall is lower in the case of the SMOTEd model (compared to the case that we omit SMOTE from the pipeline).**\n",
    "\n",
    "**Recall's formula: TP / (TP + FN).**\n",
    "\n",
    "**Recall of the SMOTEd model = 21256 / ( 21256 + 2170) = 0.907**\n",
    "\n",
    "**Recall of the model that omits SMOTE = 21910 / (21910 + 1516) = 0.935**\n",
    "\n",
    "**This is also as expected. Indeed, the model without SMOTE \"sees\" fewer 0s (zeros) than the SMOTEd model, and thus the number of FN (false negatives) is much lower in the case that we omit SMOTE.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
