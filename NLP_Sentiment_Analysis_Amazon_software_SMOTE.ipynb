{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project-4: NLP (Natural Language Processing) & Bag-of-words model\n",
    "\n",
    "**Summary: In this Jupyter notebook we perform Sentiment Analysis for the Amazon Software dataset using the Bag-of-Words model and using SMOTE (Synthetic Minority Over-sampling Technique).**\n",
    "\n",
    "The Amazon Softare dataset is obtained from this website: https://nijianmo.github.io/amazon/index.html \n",
    "\n",
    "We use two sources of data: \n",
    "\n",
    "(1) Two zip files -i.e., \"Software_df1.json.gz\" and \"Software_df2.json.gz\"- contain the main dataset. These two zip files contain the reviews of the clients and contain the rating for each product;\n",
    "\n",
    "(2) Another zip file -i.e., \"meta_Software.json.gz\"- contains the \"title\" of the product (i.e., the name of the product). It also contains the brand of the product and the main category of the product. \n",
    "\n",
    "The three datasets are merged together using the product ID which is included in all datasets. \n",
    "\n",
    "Description of the main variables in the main dataset: \n",
    "\n",
    "- reviewerID - ID of the reviewer, e.g. A2SUAM1J3GNN3B\n",
    "- asin - ID of the product, e.g. 0000013714\n",
    "- reviewerName - name of the reviewer\n",
    "- summary - summary of the review\n",
    "- reviewText - text of the review\n",
    "- overall - rating of the product\n",
    "- unixReviewTime - time of the review (unix time)\n",
    "- reviewTime - time of the review (raw)\n",
    "\n",
    "Description of the main variables in the metadata dataset (i.e., \"meta_Software.json.gz\"): \n",
    "\n",
    "- asin - ID of the product, e.g. 0000013714\n",
    "- title - name of the product\n",
    "- brand - brand name\n",
    "- main_cat - main category of the product (e.g., \"Software\"; \"All Electronics\")\n",
    "\n",
    "These sections are included in this Jupyter notebook: \n",
    "- In Section 1 we prepare the two datasets and we merge them using the product ID.  \n",
    "- In Section 2 we create the binary rating variable. The binary rating variable is used as the target of the BOW (bag-of-words) model in the following sections. \n",
    "- In Section 3 we apply pre-processing to the clients' reviews. We delete stopwords and numbers, and we use lower-case letters for all reviews; we remove punctuation, and we apply lemmatization. \n",
    "- In Section 4 we perform Sentiment Analysis. In Section 4 we use Grid search (i.e., GridSearchCV) in order to select the best model -i.e., the model with the highest predictive power. We also use a pipeline within GridSearchCV. \n",
    "- In Section 5 we use grid search together with SMOTE (Synthetic Minority Over-sampling Technique); in this section we use the unbalanced dataset, and we correct the unbalancedness of the dataset through the use of SMOTE. \n",
    "- In Section 6 we draw a few conclusions about the effect of SMOTE on the \"precision\" and the \"recall\" of the estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 1: Preparing the Amazon (software) dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing standard libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Array\n",
    "import numpy as np\n",
    "\n",
    "# Decompress the file\n",
    "import gzip\n",
    "\n",
    "## Warnings\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield json.loads(l)\n",
    "        \n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient = 'index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the main dataset which includes the reviews and the ratings: The main dataset is included in 2 ZIP files: \"Software_df1.json.gz\" and \"Software_df2.json.gz\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>A1YCCU0YRLS0FE</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "      <td>1393113600</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0      4.0      True  03 11, 2014  A240ORQ2LF9LUI  0077613252   \n",
       "1      4.0      True  02 23, 2014  A1YCCU0YRLS0FE  0077613252   \n",
       "\n",
       "                        style         reviewerName  \\\n",
       "0  {'Format:': ' Loose Leaf'}           Michelle W   \n",
       "1  {'Format:': ' Loose Leaf'}  Rosalind White Ames   \n",
       "\n",
       "                                          reviewText         summary  \\\n",
       "0  The materials arrived early and were in excell...  Material Great   \n",
       "1  I am really enjoying this book with the worksh...          Health   \n",
       "\n",
       "   unixReviewTime  vote image  \n",
       "0      1394496000  None  None  \n",
       "1      1393113600  None  None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>04 8, 2014</td>\n",
       "      <td>A27DKZ1EWN6ATA</td>\n",
       "      <td>B00E3RH3EW</td>\n",
       "      <td>{'Platform:': ' PC Download'}</td>\n",
       "      <td>Terrell Shead</td>\n",
       "      <td>I could not download the software. I guess the...</td>\n",
       "      <td>Could Not Download</td>\n",
       "      <td>1396915200</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>04 7, 2014</td>\n",
       "      <td>A2OJSZMT9L3YOF</td>\n",
       "      <td>B00E3RH3EW</td>\n",
       "      <td>{'Platform:': ' PC Download'}</td>\n",
       "      <td>FitzSimpson</td>\n",
       "      <td>THIS PROGRAM IS AWESOME. IT KEEPS MY COMPUTER ...</td>\n",
       "      <td>SATISFIED.</td>\n",
       "      <td>1396828800</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  verified  reviewTime      reviewerID        asin  \\\n",
       "0      1.0      True  04 8, 2014  A27DKZ1EWN6ATA  B00E3RH3EW   \n",
       "1      4.0      True  04 7, 2014  A2OJSZMT9L3YOF  B00E3RH3EW   \n",
       "\n",
       "                           style   reviewerName  \\\n",
       "0  {'Platform:': ' PC Download'}  Terrell Shead   \n",
       "1  {'Platform:': ' PC Download'}    FitzSimpson   \n",
       "\n",
       "                                          reviewText             summary  \\\n",
       "0  I could not download the software. I guess the...  Could Not Download   \n",
       "1  THIS PROGRAM IS AWESOME. IT KEEPS MY COMPUTER ...          SATISFIED.   \n",
       "\n",
       "   unixReviewTime  vote image  \n",
       "0      1396915200  None  None  \n",
       "1      1396828800     2  None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "review_df1 = getDF('data/Software_df1.json.gz')\n",
    "display(review_df1.head(2))\n",
    "review_df2 = getDF('data/Software_df2.json.gz')\n",
    "display(review_df2.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = review_df1.append(review_df2, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>vote</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>A1YCCU0YRLS0FE</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "      <td>1393113600</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 17, 2014</td>\n",
       "      <td>A1BJHRQDYVAY2J</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Allan R. Baker</td>\n",
       "      <td>IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...</td>\n",
       "      <td>ARE YOU KIDING ME?</td>\n",
       "      <td>1392595200</td>\n",
       "      <td>7</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>02 17, 2014</td>\n",
       "      <td>APRDVZ6QBIQXT</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>{'Format:': ' Loose Leaf'}</td>\n",
       "      <td>Lucy</td>\n",
       "      <td>This book was missing pages!!! Important pages...</td>\n",
       "      <td>missing pages!!</td>\n",
       "      <td>1392595200</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "      <td>10 14, 2013</td>\n",
       "      <td>A2JZTTBSLS1QXV</td>\n",
       "      <td>0077775473</td>\n",
       "      <td>None</td>\n",
       "      <td>Albert V.</td>\n",
       "      <td>I have used LearnSmart and can officially say ...</td>\n",
       "      <td>Best study product out there!</td>\n",
       "      <td>1381708800</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459431</th>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "      <td>11 29, 2016</td>\n",
       "      <td>AGEWYJ2NF5C2H</td>\n",
       "      <td>B01HF41TKI</td>\n",
       "      <td>None</td>\n",
       "      <td>Bonita Alferes</td>\n",
       "      <td>No instructions.....No Help unless you want to...</td>\n",
       "      <td>Two Stars</td>\n",
       "      <td>1480377600</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459432</th>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>03 1, 2018</td>\n",
       "      <td>A3VCFV8WEQG9R5</td>\n",
       "      <td>B01HF3G4BS</td>\n",
       "      <td>None</td>\n",
       "      <td>mekonen</td>\n",
       "      <td>it's a joke</td>\n",
       "      <td>One Star</td>\n",
       "      <td>1519862400</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459433</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>12 2, 2017</td>\n",
       "      <td>A3DXGHJF6SOHNC</td>\n",
       "      <td>B01HF3G4BS</td>\n",
       "      <td>None</td>\n",
       "      <td>bbeckham</td>\n",
       "      <td>I have multiple licenses of the Antivirus. I h...</td>\n",
       "      <td>This is very effective antivirus software.</td>\n",
       "      <td>1512172800</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459434</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>09 15, 2018</td>\n",
       "      <td>A1WOS4D7QA06DO</td>\n",
       "      <td>B01HJAMWOK</td>\n",
       "      <td>None</td>\n",
       "      <td>Charles E. Potter</td>\n",
       "      <td>good value</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1536969600</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459435</th>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>10 5, 2016</td>\n",
       "      <td>A20SG9ZGIIFW69</td>\n",
       "      <td>B01HJAMWOK</td>\n",
       "      <td>None</td>\n",
       "      <td>Joe</td>\n",
       "      <td>very nice designs easy to use.</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1475625600</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>459436 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        overall  verified   reviewTime      reviewerID        asin  \\\n",
       "0           4.0      True  03 11, 2014  A240ORQ2LF9LUI  0077613252   \n",
       "1           4.0      True  02 23, 2014  A1YCCU0YRLS0FE  0077613252   \n",
       "2           1.0      True  02 17, 2014  A1BJHRQDYVAY2J  0077613252   \n",
       "3           3.0      True  02 17, 2014   APRDVZ6QBIQXT  0077613252   \n",
       "4           5.0     False  10 14, 2013  A2JZTTBSLS1QXV  0077775473   \n",
       "...         ...       ...          ...             ...         ...   \n",
       "459431      2.0      True  11 29, 2016   AGEWYJ2NF5C2H  B01HF41TKI   \n",
       "459432      1.0      True   03 1, 2018  A3VCFV8WEQG9R5  B01HF3G4BS   \n",
       "459433      5.0      True   12 2, 2017  A3DXGHJF6SOHNC  B01HF3G4BS   \n",
       "459434      5.0      True  09 15, 2018  A1WOS4D7QA06DO  B01HJAMWOK   \n",
       "459435      5.0      True   10 5, 2016  A20SG9ZGIIFW69  B01HJAMWOK   \n",
       "\n",
       "                             style         reviewerName  \\\n",
       "0       {'Format:': ' Loose Leaf'}           Michelle W   \n",
       "1       {'Format:': ' Loose Leaf'}  Rosalind White Ames   \n",
       "2       {'Format:': ' Loose Leaf'}       Allan R. Baker   \n",
       "3       {'Format:': ' Loose Leaf'}                 Lucy   \n",
       "4                             None            Albert V.   \n",
       "...                            ...                  ...   \n",
       "459431                        None       Bonita Alferes   \n",
       "459432                        None              mekonen   \n",
       "459433                        None             bbeckham   \n",
       "459434                        None    Charles E. Potter   \n",
       "459435                        None                  Joe   \n",
       "\n",
       "                                               reviewText  \\\n",
       "0       The materials arrived early and were in excell...   \n",
       "1       I am really enjoying this book with the worksh...   \n",
       "2       IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...   \n",
       "3       This book was missing pages!!! Important pages...   \n",
       "4       I have used LearnSmart and can officially say ...   \n",
       "...                                                   ...   \n",
       "459431  No instructions.....No Help unless you want to...   \n",
       "459432                                        it's a joke   \n",
       "459433  I have multiple licenses of the Antivirus. I h...   \n",
       "459434                                         good value   \n",
       "459435                     very nice designs easy to use.   \n",
       "\n",
       "                                           summary  unixReviewTime  vote image  \n",
       "0                                   Material Great      1394496000  None  None  \n",
       "1                                           Health      1393113600  None  None  \n",
       "2                               ARE YOU KIDING ME?      1392595200     7  None  \n",
       "3                                  missing pages!!      1392595200     3  None  \n",
       "4                    Best study product out there!      1381708800  None  None  \n",
       "...                                            ...             ...   ...   ...  \n",
       "459431                                   Two Stars      1480377600  None  None  \n",
       "459432                                    One Star      1519862400  None  None  \n",
       "459433  This is very effective antivirus software.      1512172800  None  None  \n",
       "459434                                  Five Stars      1536969600  None  None  \n",
       "459435                                  Five Stars      1475625600  None  None  \n",
       "\n",
       "[459436 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicate records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(421433, 12)\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### Drop duplicated records\n",
    "###########################\n",
    "review_df = review_df.drop_duplicates(subset='reviewText', keep='first')\n",
    "print(review_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### Rename column \"overall\" to \"Rating\" \n",
    "###########################\n",
    "review_df = review_df.rename(columns={'overall':'Rating'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### Save it as CSV file\n",
    "###########################\n",
    "review_df.to_csv('data/review_df.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the metadata to extract the title, brand and main category of the product. The \"title\" is the name of the product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26790, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>tech1</th>\n",
       "      <th>description</th>\n",
       "      <th>fit</th>\n",
       "      <th>title</th>\n",
       "      <th>also_buy</th>\n",
       "      <th>image</th>\n",
       "      <th>tech2</th>\n",
       "      <th>brand</th>\n",
       "      <th>feature</th>\n",
       "      <th>rank</th>\n",
       "      <th>also_view</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>similar_item</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>asin</th>\n",
       "      <th>details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>HOLT PHYSICS LESSON PRESENTATION CD-ROM QUICK ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>HOLT. RINEHART AND WINSTON</td>\n",
       "      <td>[]</td>\n",
       "      <td>25,550 in Software (</td>\n",
       "      <td>[]</td>\n",
       "      <td>Software</td>\n",
       "      <td></td>\n",
       "      <td>&lt;/div&gt;</td>\n",
       "      <td>.a-box-inner{background-color:#fff}#alohaBuyBo...</td>\n",
       "      <td>0030672120</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>[, &lt;b&gt;Latin rhythms that will get your kids si...</td>\n",
       "      <td></td>\n",
       "      <td>Sing, Watch, &amp;amp; Learn Spanish (DVD + Guide)...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td></td>\n",
       "      <td>McGraw Hill</td>\n",
       "      <td>[]</td>\n",
       "      <td>15,792 in Software (</td>\n",
       "      <td>[]</td>\n",
       "      <td>Software</td>\n",
       "      <td></td>\n",
       "      <td>&lt;/div&gt;</td>\n",
       "      <td></td>\n",
       "      <td>0071480935</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category tech1                                        description fit  \\\n",
       "0       []                                                       []       \n",
       "1       []        [, <b>Latin rhythms that will get your kids si...       \n",
       "\n",
       "                                               title also_buy  \\\n",
       "0  HOLT PHYSICS LESSON PRESENTATION CD-ROM QUICK ...       []   \n",
       "1  Sing, Watch, &amp; Learn Spanish (DVD + Guide)...       []   \n",
       "\n",
       "                                               image tech2  \\\n",
       "0                                                 []         \n",
       "1  [https://images-na.ssl-images-amazon.com/image...         \n",
       "\n",
       "                        brand feature                  rank also_view  \\\n",
       "0  HOLT. RINEHART AND WINSTON      []  25,550 in Software (        []   \n",
       "1                 McGraw Hill      []  15,792 in Software (        []   \n",
       "\n",
       "   main_cat similar_item    date  \\\n",
       "0  Software               </div>   \n",
       "1  Software               </div>   \n",
       "\n",
       "                                               price        asin details  \n",
       "0  .a-box-inner{background-color:#fff}#alohaBuyBo...  0030672120     NaN  \n",
       "1                                                     0071480935     NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = getDF('data/meta_Software.json.gz')\n",
    "print(metadata.shape)\n",
    "metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21110, 18)\n"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "### Drop duplicated records\n",
    "###########################\n",
    "metadata = metadata.drop_duplicates(subset='title', keep='first')\n",
    "print(metadata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### Keep only 'title', 'brand' and 'asin' (i.e., the product ID) for the metadata dataframe \n",
    "###########################\n",
    "metadata = metadata[['title', 'brand', 'asin', 'main_cat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "### Save it as CSV file\n",
    "###########################\n",
    "metadata.to_csv('data/metadata.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating                 0\n",
      "verified               0\n",
      "reviewTime             0\n",
      "reviewerID             0\n",
      "asin                   0\n",
      "style             206584\n",
      "reviewerName          21\n",
      "reviewText             1\n",
      "summary               40\n",
      "unixReviewTime         0\n",
      "vote              298680\n",
      "image             419964\n",
      "dtype: int64\n",
      "\n",
      "title       0\n",
      "brand       0\n",
      "asin        0\n",
      "main_cat    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "####  Data cleaning: check missing values \n",
    "#############################\n",
    "print(review_df.isna().sum())\n",
    "print('')\n",
    "print(metadata.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating                 0\n",
       "verified               0\n",
       "reviewTime             0\n",
       "reviewerID             0\n",
       "asin                   0\n",
       "style             206565\n",
       "reviewerName          21\n",
       "reviewText             0\n",
       "summary                0\n",
       "unixReviewTime         0\n",
       "vote              298647\n",
       "image             419923\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################\n",
    "### Drop rows with missing reviewText and summary\n",
    "################################\n",
    "review_df = review_df.dropna(subset=['reviewText', 'summary'])\n",
    "review_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We merge the two dataset using the product ID (i.e., \"asin\").** **Some of the products (i.e., \"asin\") that are included in the main dataset are not included in the metadata.** **Thus, we use inner join on \"asin\" because we want to have all information (e.g., information on \"title\", \"brand\", \"main category\") for all the reviews included in the final dataset:**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = review_df.merge(metadata, how='inner', on='asin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(410492, 15)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating                 0\n",
       "verified               0\n",
       "reviewTime             0\n",
       "reviewerID             0\n",
       "asin                   0\n",
       "style             203413\n",
       "reviewerName          21\n",
       "reviewText             0\n",
       "summary                0\n",
       "unixReviewTime         0\n",
       "vote              290118\n",
       "image             409064\n",
       "title                  0\n",
       "brand                  0\n",
       "main_cat               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### We drop columns 'image', 'vote', 'style', 'verified' as these columns are redundant and useless.  \n",
    "################################\n",
    "review_df.drop('image', inplace=True, axis=1)\n",
    "review_df.drop('vote', inplace=True, axis=1)\n",
    "review_df.drop('style', inplace=True, axis=1)\n",
    "review_df.drop('verified', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(410492, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>main_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>A1YCCU0YRLS0FE</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "      <td>1393113600</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating   reviewTime      reviewerID        asin         reviewerName  \\\n",
       "0     4.0  03 11, 2014  A240ORQ2LF9LUI  0077613252           Michelle W   \n",
       "1     4.0  02 23, 2014  A1YCCU0YRLS0FE  0077613252  Rosalind White Ames   \n",
       "\n",
       "                                          reviewText         summary  \\\n",
       "0  The materials arrived early and were in excell...  Material Great   \n",
       "1  I am really enjoying this book with the worksh...          Health   \n",
       "\n",
       "   unixReviewTime                                              title  \\\n",
       "0      1394496000  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "1      1393113600  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "\n",
       "                                              brand  main_cat  \n",
       "0  McGraw-Hill Humanities/Social Sciences/Languages  Software  \n",
       "1  McGraw-Hill Humanities/Social Sciences/Languages  Software  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(review_df.shape)\n",
    "review_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are lots of categories (i.e., main_cat) which are not strictly related to Software. Since we want to focus only on software products, we discard observations for products that do not belong to the category of software:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Software', 'Books', 'Video Games', 'Movies &amp; TV',\n",
       "       'Cell Phones &amp; Accessories', 'Office Products',\n",
       "       'Toys &amp; Games', 'All Electronics', 'Cell Phones & Accessories',\n",
       "       'GPS & Navigation', 'Movies & TV', 'Toys & Games',\n",
       "       'Musical Instruments', 'GPS &amp; Navigation',\n",
       "       'Arts, Crafts &amp; Sewing', 'Home Audio &amp; Theater',\n",
       "       'Camera & Photo', 'Car Electronics', 'Arts, Crafts & Sewing',\n",
       "       'Home Audio & Theater', 'Computers', 'Tools & Home Improvement',\n",
       "       'Amazon Home', 'Camera &amp; Photo', 'Baby', 'Pet Supplies',\n",
       "       'Tools &amp; Home Improvement',\n",
       "       '<img src=\"https://images-na.ssl-images-amazon.com/images/G/01/digital/music/logos/amzn_music_logo_subnav._CB471835632_.png\" class=\"nav-categ-image\" alt=\"Digital Music\"/>',\n",
       "       'Automotive', 'Health & Personal Care', 'Sports &amp; Outdoors',\n",
       "       'Health &amp; Personal Care', 'Sports & Outdoors',\n",
       "       '<img src=\"https://m.media-amazon.com/images/G/01/digital/music/logos/amzn_music_logo_subnav._CB471835632_.png\" class=\"nav-categ-image\" alt=\"Digital Music\"/>',\n",
       "       'Audible Audiobooks', 'Industrial & Scientific',\n",
       "       'Industrial &amp; Scientific', '',\n",
       "       '<img src=\"https://images-na.ssl-images-amazon.com/images/G/01/nav2/images/gui/amazon-fashion-store-new._CB520838675_.png\" class=\"nav-categ-image\" alt=\"AMAZON FASHION\"/>',\n",
       "       'All Beauty', 'Grocery', 'Portable Audio & Accessories',\n",
       "       '<img src=\"https://images-na.ssl-images-amazon.com/images/G/01/digital/music/logos/amzn_music_logo_subnav._CB471835632_.png\" class=\"nav-categ-image\" alt=\"Digital Music\" />'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.main_cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df = review_df[review_df.main_cat == 'Software']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(389927, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>main_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>A1YCCU0YRLS0FE</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "      <td>1393113600</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating   reviewTime      reviewerID        asin         reviewerName  \\\n",
       "0     4.0  03 11, 2014  A240ORQ2LF9LUI  0077613252           Michelle W   \n",
       "1     4.0  02 23, 2014  A1YCCU0YRLS0FE  0077613252  Rosalind White Ames   \n",
       "\n",
       "                                          reviewText         summary  \\\n",
       "0  The materials arrived early and were in excell...  Material Great   \n",
       "1  I am really enjoying this book with the worksh...          Health   \n",
       "\n",
       "   unixReviewTime                                              title  \\\n",
       "0      1394496000  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "1      1393113600  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "\n",
       "                                              brand  main_cat  \n",
       "0  McGraw-Hill Humanities/Social Sciences/Languages  Software  \n",
       "1  McGraw-Hill Humanities/Social Sciences/Languages  Software  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(review_df.shape)\n",
    "review_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create the client's review text which is the ensemble of 'summary' and 'reviewText':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>review_2.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "      <td>Material Great The materials arrived early and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating   reviewTime      reviewerID        asin reviewerName  \\\n",
       "0     4.0  03 11, 2014  A240ORQ2LF9LUI  0077613252   Michelle W   \n",
       "\n",
       "                                          reviewText         summary  \\\n",
       "0  The materials arrived early and were in excell...  Material Great   \n",
       "\n",
       "   unixReviewTime                                              title  \\\n",
       "0      1394496000  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "\n",
       "                                              brand  main_cat  \\\n",
       "0  McGraw-Hill Humanities/Social Sciences/Languages  Software   \n",
       "\n",
       "                                          review_2.0  \n",
       "0  Material Great The materials arrived early and...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['review_2.0'] = review_df['summary'] + \" \" + review_df['reviewText'] \n",
    "review_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 2: we create the binary rating variable. The binary rating variable is used as the target in the following sections* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0    171743\n",
       "1.0     93061\n",
       "4.0     62584\n",
       "3.0     34306\n",
       "2.0     28233\n",
       "Name: Rating, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use rating = {1,2,3} as bad/neutral rating, and we use rating = {4,5} as good rating. \n",
    "See the discussion at https://sellercentral.amazon.com/forums/t/does-a-neutral-3-star-rating-on-your-feedback-count-against-odr/1081/14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>review_2.0</th>\n",
       "      <th>Rating_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "      <td>Material Great The materials arrived early and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating   reviewTime      reviewerID        asin reviewerName  \\\n",
       "0     4.0  03 11, 2014  A240ORQ2LF9LUI  0077613252   Michelle W   \n",
       "\n",
       "                                          reviewText         summary  \\\n",
       "0  The materials arrived early and were in excell...  Material Great   \n",
       "\n",
       "   unixReviewTime                                              title  \\\n",
       "0      1394496000  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "\n",
       "                                              brand  main_cat  \\\n",
       "0  McGraw-Hill Humanities/Social Sciences/Languages  Software   \n",
       "\n",
       "                                          review_2.0  Rating_binary  \n",
       "0  Material Great The materials arrived early and...              1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['Rating_binary'] = review_df['Rating'].apply(lambda x: 0 if x < 4 else 1)\n",
    "review_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    234327\n",
       "0    155600\n",
       "Name: Rating_binary, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df['Rating_binary'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 3: Apply pre-processing to the clients' reviews* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-processing is applied in order to reduce the \"noise\" in the reviews. E.g., see https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_symbols(mystring):\n",
    "    new_string = mystring.replace(r'\\n', ' ')\n",
    "    return new_string \n",
    "\n",
    "\n",
    "# remove numbers: \n",
    "def remove_nmbrs(mystring):\n",
    "    mystring_no_numbers = ''.join(word for word in mystring if not word.isdigit())\n",
    "    return mystring_no_numbers \n",
    "\n",
    "\n",
    "def stringify(mystring):\n",
    "    new_string = str(mystring) \n",
    "    return new_string \n",
    "\n",
    "\n",
    "\n",
    "# Remove Punctuation:\n",
    "def punct(mystring):\n",
    "    import string \n",
    "    list_s_p = string.punctuation\n",
    "    for punctuation in list_s_p:\n",
    "        mystring = mystring.replace(punctuation, ' ')\n",
    "    return mystring\n",
    "\n",
    "\n",
    "# lower-case: \n",
    "def lower_it(mystring):\n",
    "    lowered_mystring = mystring.lower()\n",
    "    return lowered_mystring \n",
    "\n",
    "\n",
    "\n",
    "#Remove StopWords:\n",
    "def stopwords(mystring):\n",
    "    from nltk.corpus import stopwords \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(mystring) \n",
    "    splitting_string = [w for w in word_tokens if not w in stop_words] #creates a list!\n",
    "    text  = ' '.join(word for word in splitting_string)\n",
    "    return text \n",
    " \n",
    "\n",
    "#Lemmatize:\n",
    "def lem(mystring):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    splitting_text = mystring.split()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in splitting_text]\n",
    "    mystring = ' '.join(word for word in lemmatized)\n",
    "    return mystring\n",
    "\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = remove_symbols(text)\n",
    "    text = remove_nmbrs(text)\n",
    "    text = stringify(text)\n",
    "\n",
    "    text = punct(text)\n",
    "    text = lower_it(text)\n",
    "    text = stopwords(text)\n",
    "    text = lem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df['review_2.0'] =  review_df['review_2.0'].map(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>main_cat</th>\n",
       "      <th>review_2.0</th>\n",
       "      <th>Rating_binary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>03 11, 2014</td>\n",
       "      <td>A240ORQ2LF9LUI</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Michelle W</td>\n",
       "      <td>The materials arrived early and were in excell...</td>\n",
       "      <td>Material Great</td>\n",
       "      <td>1394496000</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "      <td>material great material arrived early excellen...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>02 23, 2014</td>\n",
       "      <td>A1YCCU0YRLS0FE</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Rosalind White Ames</td>\n",
       "      <td>I am really enjoying this book with the worksh...</td>\n",
       "      <td>Health</td>\n",
       "      <td>1393113600</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "      <td>health really enjoying book worksheet make rev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>02 17, 2014</td>\n",
       "      <td>A1BJHRQDYVAY2J</td>\n",
       "      <td>0077613252</td>\n",
       "      <td>Allan R. Baker</td>\n",
       "      <td>IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...</td>\n",
       "      <td>ARE YOU KIDING ME?</td>\n",
       "      <td>1392595200</td>\n",
       "      <td>Connect Personal Health with LearnSmart 1 Seme...</td>\n",
       "      <td>McGraw-Hill Humanities/Social Sciences/Languages</td>\n",
       "      <td>Software</td>\n",
       "      <td>kiding taking class waste money called book bo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating   reviewTime      reviewerID        asin         reviewerName  \\\n",
       "0     4.0  03 11, 2014  A240ORQ2LF9LUI  0077613252           Michelle W   \n",
       "1     4.0  02 23, 2014  A1YCCU0YRLS0FE  0077613252  Rosalind White Ames   \n",
       "2     1.0  02 17, 2014  A1BJHRQDYVAY2J  0077613252       Allan R. Baker   \n",
       "\n",
       "                                          reviewText             summary  \\\n",
       "0  The materials arrived early and were in excell...      Material Great   \n",
       "1  I am really enjoying this book with the worksh...              Health   \n",
       "2  IF YOU ARE TAKING THIS CLASS DON\"T WASTE YOUR ...  ARE YOU KIDING ME?   \n",
       "\n",
       "   unixReviewTime                                              title  \\\n",
       "0      1394496000  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "1      1393113600  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "2      1392595200  Connect Personal Health with LearnSmart 1 Seme...   \n",
       "\n",
       "                                              brand  main_cat  \\\n",
       "0  McGraw-Hill Humanities/Social Sciences/Languages  Software   \n",
       "1  McGraw-Hill Humanities/Social Sciences/Languages  Software   \n",
       "2  McGraw-Hill Humanities/Social Sciences/Languages  Software   \n",
       "\n",
       "                                          review_2.0  Rating_binary  \n",
       "0  material great material arrived early excellen...              1  \n",
       "1  health really enjoying book worksheet make rev...              1  \n",
       "2  kiding taking class waste money called book bo...              0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 4: We search the best model in this section: We use grid search using a pipeline which is composed by (1) TfidfVectorizer; (2) Algorithm for classification* \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section we use a balanced dataset: i.e., the number of 0s for the target variable is the same as the number of 1s: We use a balanced dataset because most machine learning algorithms do not work very well with imbalanced datasets. For more information on this issue, see https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(389927, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    234327\n",
       "0    155600\n",
       "Name: Rating_binary, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df_balanced = review_df.copy()\n",
    "print(review_df_balanced.shape)\n",
    "review_df_balanced['Rating_binary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155600\n",
      "1    155600\n",
      "0    155600\n",
      "Name: Rating_binary, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "#### Balancing the dataframe: \n",
    "################################\n",
    "\n",
    "num = (review_df_balanced['Rating_binary'].value_counts()).min()\n",
    "print(num )\n",
    "#155600\n",
    "\n",
    "df_pos = review_df_balanced[review_df_balanced['Rating_binary'] == 1].sample(num, random_state=0)\n",
    "df_neg = review_df_balanced[review_df_balanced['Rating_binary'] == 0].sample(num)\n",
    "review_df_balanced = pd.concat([df_pos, df_neg], verify_integrity=True)\n",
    "print(review_df_balanced['Rating_binary'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Baseline Estimator (i.e., \"DummyClassifier\" algorithm):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "{}\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from imblearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000) \n",
    "dummy = DummyClassifier()  \n",
    "\n",
    "pipeline_dummy = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('dummy', dummy)])\n",
    "parameters = {}\n",
    "\n",
    "# Perform simply CV (Cross-validation) without searching the best parameters:\n",
    "grid_search = GridSearchCV(pipeline_dummy, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_balanced['review_2.0']\n",
    "y = review_df_balanced['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "#0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The accuracy of the baseline estimator is 0.5: I.e., only 50% of the target is correctly predicted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes algorithm with grid search (grid search is used to identify the best \"alpha\" parameter of Naive Bayes algorithm):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "{'nb__alpha': 1}\n",
      "0.8554016709511568\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000) \n",
    "naive_bayes = MultinomialNB()  \n",
    "\n",
    "pipeline_dummy = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('nb', naive_bayes)])\n",
    "\n",
    "parameters = {'nb__alpha': (0, 0.01, 0.05, 0.1, 0.3, 0.5, 1),\n",
    "              }\n",
    "\n",
    "# Perform CV (Cross-validation) together with grid search:\n",
    "grid_search = GridSearchCV(pipeline_dummy, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_balanced['review_2.0']\n",
    "y = review_df_balanced['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "#Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
    "#{'nb__alpha': 1}\n",
    "#0.855"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best accuracy of the Naive Bayes estimator is 0.855: I.e., 85.5% of the target is correctly predicted. This is a large improvement with respect to the baseline estimator.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with grid search (grid search is used to identify the best \"penalty\" parameter of Logistic regression ): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "{'log__penalty': 'l2'}\n",
      "0.9003566838046272\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000) \n",
    "log_reg = LogisticRegression()  \n",
    "\n",
    "pipeline_dummy = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('log', log_reg)])\n",
    "\n",
    "parameters = {'log__penalty': ['l1','l2']\n",
    "              }\n",
    "# Perform CV (Cross-validation) together with grid search:\n",
    "grid_search = GridSearchCV(pipeline_dummy, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_balanced['review_2.0']\n",
    "y = review_df_balanced['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "#Fitting 5 folds for each of 2 candidates, totalling 10 fits \n",
    "#{'log__penalty': 'l2'}\n",
    "#0.900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best accuracy of the Logistic Regression is around 0.90: I.e., around 90% of the target is correctly predicted. This is a fairly large improvement with respect to the baseline estimator.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree algorithm with grid search (grid search is used to identify the best \"max_depth\"  parameter of the decision tree algorithm): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "{'tree__max_depth': 8}\n",
      "0.7578374035989717\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000) \n",
    "tree = DecisionTreeClassifier()  \n",
    "\n",
    "pipeline_dummy = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('tree', tree)])\n",
    "\n",
    "parameters = {'tree__max_depth': [2, 3, 6, 8]\n",
    "              }\n",
    "\n",
    "# Perform  CV (Cross-validation): \n",
    "grid_search = GridSearchCV(pipeline_dummy, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_balanced['review_2.0']\n",
    "y = review_df_balanced['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "#Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
    "#{'tree__max_depth': 8}\n",
    "#0.757"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The performance of the decision tree is worse than Logistic Regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost (Extreme Gradient Boosting) classifier algorithm with grid search (grid search is used to identify the best \"max_depth\"  parameter of the XGBoost algorithm): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[14:01:35] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "{'xgboost__max_depth': 7}\n",
      "0.7877988431876606\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from xgboost import XGBClassifier     \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000) \n",
    "xgboost = XGBClassifier(learning_rate=0.02, n_estimators=100, objective='binary:logistic',\n",
    "                    silent=True, nthread=-1)\n",
    "\n",
    "pipeline_dummy = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('xgboost', xgboost)])\n",
    "\n",
    "parameters = {'xgboost__max_depth': [6,7]\n",
    "              }\n",
    "\n",
    "# Perform CV (Cross-validation):\n",
    "grid_search = GridSearchCV(pipeline_dummy, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_balanced['review_2.0']\n",
    "y = review_df_balanced['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "#Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
    "#{'xgboost__max_depth': 7}\n",
    "#0.787"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The performance of XGBoost classifier is worse than Logistic Regression.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: Logistic Regression is the best algorithm for our task. Moreover, the Lasso penalty -i.e., l2-penalty- is the best penalty for Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thus, we save the Logistic Regression model with Lasso penalty as our favorite estimator. We use joblib (library) to save the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000) \n",
    "log_reg = LogisticRegression()  \n",
    "\n",
    "pipeline_dummy = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('log', log_reg)])\n",
    "\n",
    "parameters = {'log__penalty': ['l1','l2']\n",
    "              }\n",
    "# Perform simply CV (Cross-validation) without searching the best parameters:\n",
    "grid_search = GridSearchCV(pipeline_dummy, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_balanced['review_2.0']\n",
    "y = review_df_balanced['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finalized_model.joblib']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.joblib'\n",
    "#joblib.dump(grid_search.best_estimator_, filename, compress = 1)\n",
    "joblib.dump(grid_search.best_estimator_, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Out-of-sample prediction: We use two reviews from Amazon UK (Software) to further test our model (The above model was estimated using data from Amazon USA)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of review with bad rating:** \n",
    "\n",
    "\"When I received this product, I thought I would get the actual disk from Microsoft. However, it was a burned disk with instructions on how to download it and activate it. The instructions were very confusing and when I put in my product key, it said it was not a valid key. Got on the phone and could get no help, so had to send it back and buy from someone else. Very happy with the new product which was office 2010. If you buy this hope you have better luck than me.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The rating for the above-mentioned review is 2 stars. That is, the customer dislikes the product.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The review is taken from Amazon UK (software):**\n",
    "https://www.amazon.co.uk/Microsoft-Professional-Genuine-Lifetime-Product/dp/B00WYPDA4C/ref=sr_1_16?dchild=1&keywords=microsoft+office&qid=1608730193&s=software&sr=1-16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = joblib.load(filename)\n",
    "\n",
    "#I use one example of a review from Amazon UK (Software): https://www.amazon.co.uk/Microsoft-Professional-Genuine-Lifetime-Product/dp/B00WYPDA4C/ref=sr_1_16?dchild=1&keywords=microsoft+office&qid=1608730193&s=software&sr=1-16\n",
    "#This review has a bad rating (i.e., 2 stars):\n",
    "example_of_review = 'When I received this product, I thought I would get the actual disk from Microsoft. However, it was a burned disk with instructions on how to download it and activate it. The instructions were very confusing and when I put in my product key, it said it was not a valid key. Got on the phone and could get no help, so had to send it back and buy from someone else. Very happy with the new product which was office 2010. If you buy this hope you have better luck than me.'\n",
    "\n",
    "#Apply the preprocessing: \n",
    "example_of_review =  preprocessing(example_of_review)\n",
    "\n",
    "#Transform the \"review\" into a list (or an iterable) containing a single element: \n",
    "final_review = [example_of_review]\n",
    "result = loaded_model.predict(final_review)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of review with good rating:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"I don't usually write reviews but this compnay deserves a big shout our for great customer service. I am not computer savey and was really messing up installation, so i called the tech support and ended up talking to Mitchell he was patient and didn't make me feel like and idoit for messing things up. If i hadn't been for him this review would have been much different. He went out of his way to make sure eveything was installed right and even made called microsoft because i had messed things up so bad. Thank you thank you for companys with intergrity and great customer service.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The review is taken from Amazon UK (software):** \"https://www.amazon.co.uk/product-reviews/B00WYPDA4C/ref=acr_dp_hist_5?ie=UTF8&filterByStar=five_star&reviewerType=all_reviews#reviews-filter-bar\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The rating for the above-mentioned review is 5 stars. That is, the customer likes the product.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = joblib.load(filename)\n",
    "\n",
    "#I use one example of a review from Amazon UK (Software): https://www.amazon.co.uk/Microsoft-Professional-Genuine-Lifetime-Product/dp/B00WYPDA4C/ref=sr_1_16?dchild=1&keywords=microsoft+office&qid=1608730193&s=software&sr=1-16\n",
    "#This review has a good rating (i.e., 5 stars):\n",
    "example_of_review = 'I don\\'t usually write reviews but this compnay deserves a big shout our for great customer service. I am not computer savey and was really messing up installation, so i called the tech support and ended up talking to Mitchell he was patient and didn\\'t make me feel like and idoit for messing things up. If i hadn\\'t been for him this review would have been much different. He went out of his way to make sure eveything was installed right and even made called microsoft because i had messed things up so bad. Thank you thank you for companys with intergrity and great customer service.'\n",
    "\n",
    "#Apply the preprocessing: \n",
    "example_of_review =  preprocessing(example_of_review)\n",
    "\n",
    "#Transform the \"review\" into a list (or an iterable) containing a single element: \n",
    "final_review = [example_of_review]\n",
    "result = loaded_model.predict(final_review)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion for the two out-of-sample predictions: both cases are correctly predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 5: Grid Search using a pipeline which is composed by (1) TfidfVectorizer; (2) SMOTE (Synthetic Minority Over-sampling Technique); (3) Logistic regression.* \n",
    "\n",
    "**In this section we use the unbalanced dataset, and we do not balance the dataset because we use SMOTE.** **We use only 10% of the original (unbalanced) dataset, because of memory issues which happened during the estimation of the full (unbalanced) dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use the best model that was found in Section 4, but we additionally use SMOTE on the unbalanced dataset. In order to use SMOTE, we need to use the unbalanced dataset which contains 10% of the original observations (i.e., review_df_restricted). See below for more details about review_df_restricted. All other settings are exactly the same as for our preferred model from Section 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(389927, 13)\n",
      "(38992, 13)\n"
     ]
    }
   ],
   "source": [
    "print(review_df.shape)\n",
    "#(389927, 13)\n",
    "\n",
    "import math \n",
    "n_obs = int(math.floor(389927/10))\n",
    "review_df_restricted  = review_df.sample(n_obs, random_state=1)\n",
    "print(review_df_restricted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    23426\n",
       "0    15566\n",
       "Name: Rating_binary, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pre-processing was already applid above for the full dataset. \n",
    "review_df_restricted['Rating_binary'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "{'log_reg__penalty': 'l2'}\n",
      "0.8764619239989765\n"
     ]
    }
   ],
   "source": [
    "# GridsearchCV adding SMOTE in the pipeline:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000)\n",
    "log_reg = LogisticRegression() \n",
    "\n",
    "pipeline_log_reg = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('sampling', SMOTE()),\n",
    "        ('log_reg', log_reg)])\n",
    "\n",
    "parameters = {'log_reg__penalty': ['l2']} \n",
    "\n",
    "# Perform cross validation: \n",
    "grid_search = GridSearchCV(pipeline_log_reg, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"accuracy\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_restricted['review_2.0']\n",
    "y = review_df_restricted['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also save our model (which includes SMOTE within the pipeline) in a joblib file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finalized_model_SMOTE.joblib']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# save the model to disk\n",
    "filename = 'finalized_model_SMOTE.joblib'\n",
    "#joblib.dump(grid_search.best_estimator_, filename, compress = 1)\n",
    "joblib.dump(grid_search.best_estimator_, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample prediction: We use two reviews from Amazon UK (Software) website to further test our model: These two reviews are the same as in Section 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First review and first prediction. The customer's review is the following one:**\n",
    "\n",
    "\"When I received this product, I thought I would get the actual disk from Microsoft. However, it was a burned disk with instructions on how to download it and activate it. The instructions were very confusing and when I put in my product key, it said it was not a valid key. Got on the phone and could get no help, so had to send it back and buy from someone else. Very happy with the new product which was office 2010. If you buy this hope you have better luck than me.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = joblib.load('finalized_model_SMOTE.joblib')\n",
    "\n",
    "#I use one example of a review from Amazon UK (Software): https://www.amazon.co.uk/Microsoft-Professional-Genuine-Lifetime-Product/dp/B00WYPDA4C/ref=sr_1_16?dchild=1&keywords=microsoft+office&qid=1608730193&s=software&sr=1-16\n",
    "#This review has a bad rating (i.e., 2 stars):\n",
    "example_of_review = 'When I received this product, I thought I would get the actual disk from Microsoft. However, it was a burned disk with instructions on how to download it and activate it. The instructions were very confusing and when I put in my product key, it said it was not a valid key. Got on the phone and could get no help, so had to send it back and buy from someone else. Very happy with the new product which was office 2010. If you buy this hope you have better luck than me.'\n",
    "\n",
    "#Apply the preprocessing: \n",
    "example_of_review =  preprocessing(example_of_review)\n",
    "\n",
    "#Transform the \"review\" into a list (or an iterable) containing a single element: \n",
    "final_review = [example_of_review]\n",
    "result = loaded_model.predict(final_review)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second review and second prediction. The customer's review is the following one:**\n",
    "\n",
    "\"I don't usually write reviews but this compnay deserves a big shout our for great customer service. I am not computer savey and was really messing up installation, so i called the tech support and ended up talking to Mitchell he was patient and didn't make me feel like and idoit for messing things up. If i hadn't been for him this review would have been much different. He went out of his way to make sure eveything was installed right and even made called microsoft because i had messed things up so bad. Thank you thank you for companys with intergrity and great customer service.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = joblib.load('finalized_model_SMOTE.joblib')\n",
    "\n",
    "#I use one example of a review from Amazon UK (Software): https://www.amazon.co.uk/Microsoft-Professional-Genuine-Lifetime-Product/dp/B00WYPDA4C/ref=sr_1_16?dchild=1&keywords=microsoft+office&qid=1608730193&s=software&sr=1-16\n",
    "#This review has a good rating (i.e., 5 stars):\n",
    "example_of_review = 'I don\\'t usually write reviews but this compnay deserves a big shout our for great customer service. I am not computer savey and was really messing up installation, so i called the tech support and ended up talking to Mitchell he was patient and didn\\'t make me feel like and idoit for messing things up. If i hadn\\'t been for him this review would have been much different. He went out of his way to make sure eveything was installed right and even made called microsoft because i had messed things up so bad. Thank you thank you for companys with intergrity and great customer service.'\n",
    "\n",
    "#Apply the preprocessing: \n",
    "example_of_review =  preprocessing(example_of_review)\n",
    "\n",
    "#Transform the \"review\" into a list (or an iterable) containing a single element: \n",
    "final_review = [example_of_review]\n",
    "result = loaded_model.predict(final_review)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion for out-of-sample predictions: both reviews are correctly predicted by our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 6: we draw a few conclusions about the effect of SMOTE on the \"precision\" and the \"recall\" of the estimator.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 6.1) SMOTE is used to \"balance\" the unbalanced dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We use \"precision\" as the scoring measure  (and we do not use accuracy), and we estimate the model using SMOTE.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "{'log_reg__penalty': 'l2'}\n",
      "0.9107987821384562\n"
     ]
    }
   ],
   "source": [
    "# gridsearchCV:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000)\n",
    "log_reg = LogisticRegression() \n",
    "\n",
    "pipeline_log_reg = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        ('sampling', SMOTE()),\n",
    "        ('log_reg', log_reg)])\n",
    "\n",
    "parameters = {'log_reg__penalty': ['l2']} \n",
    "\n",
    "# Perform cross validation: \n",
    "grid_search = GridSearchCV(pipeline_log_reg, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"precision\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_restricted['review_2.0']\n",
    "y = review_df_restricted['Rating_binary']\n",
    "\n",
    "grid_search.fit(X,y)\n",
    "grid_search.best_params_\n",
    "grid_search.best_score_\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "\n",
    "# precision with SMOTE: \n",
    "#{'log_reg__penalty': 'l2'}\n",
    "#0.9107987821384562"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14128,  1438],\n",
       "       [ 2170, 21256]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = grid_search.best_estimator_.predict(X)\n",
    "confusion_matrix(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: the \"precision\" is 0.910 using SMOTE and the confusion matrix is reported above.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 6.2) SMOTE is OMITTED in section 6.2.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We use \"precision\" as the scoring measure  (and we do not use accuracy), and we estimate the model without SMOTE.** \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "{'log_reg__penalty': 'l2'}\n",
      "0.8900174434732577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[13517,  2049],\n",
       "       [ 1516, 21910]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gridsearchCV:\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "count_vect = TfidfVectorizer(ngram_range=(1, 1), min_df=1,  max_df = 1000000)\n",
    "log_reg = LogisticRegression() \n",
    "\n",
    "pipeline_log_reg = Pipeline(steps=[\n",
    "        ('vectorizer', count_vect),\n",
    "        #('sampling', SMOTE()),\n",
    "        ('log_reg', log_reg)])\n",
    "\n",
    "parameters = {'log_reg__penalty': ['l2']} \n",
    "\n",
    "# Perform cross validation: \n",
    "grid_WITHOUT_SMOTE = GridSearchCV(pipeline_log_reg, parameters, n_jobs=-1, \n",
    "                           verbose=1, scoring = \"precision\", \n",
    "                           refit=True, cv=5)\n",
    "\n",
    "X = review_df_restricted['review_2.0']\n",
    "y = review_df_restricted['Rating_binary']\n",
    "\n",
    "grid_WITHOUT_SMOTE.fit(X,y)\n",
    "grid_WITHOUT_SMOTE.best_params_\n",
    "grid_WITHOUT_SMOTE.best_score_\n",
    "print(grid_WITHOUT_SMOTE.best_params_)\n",
    "print(grid_WITHOUT_SMOTE.best_score_)\n",
    "\n",
    "#Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
    "#{'log_reg__penalty': 'l2'}\n",
    "#0.8900174434732577"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13517,  2049],\n",
       "       [ 1516, 21910]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred_WITHOUT_SMOTE = grid_WITHOUT_SMOTE.best_estimator_.predict(X)\n",
    "confusion_matrix(y, y_pred_WITHOUT_SMOTE)\n",
    "\n",
    "#array([[13517,  2049],\n",
    "#       [ 1516, 21910]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: the \"precision\" is 0.890 when we omit SMOTE from our pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Section 6.3) Comparison of the estimator with and without SMOTE.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The \"precision\" of the estimator is higher in the case that we use SMOTE (compared to the case that we omit SMOTE from the pipeline).**\n",
    "\n",
    "### Explanation/interpretation: \n",
    "- Precision's formula: TP / (TP + FP), where TP is the number of true positives and where FP is the number of false positives. \n",
    "- The dataset (review_df_restricted) contains many more 1s than 0s (zeros).\n",
    "- SMOTE forces the estimator to see more 0s (zeros) as the target variable (compared to the case that we omit SMOTE from the pipeline). \n",
    "- As a result of the above bullet point, we have that the number of FP (false positives) decreases drastically for the model that includes SMOTE (compared to the case that we omit SMOTE from the pipeline). This is exactly what we would expect from the use of SMOTE in our case!\n",
    "- In conclusion, SMOTE forces the estimator to see more 0s (zeros), and this implies that the number of FP (false positives) decreases by almost 25% compared to the the number of FP in the original model. Finally, the fact that the number of FP is much lower for the model which includes SMOTE implies that the \"Precision\" is higher for the same model. This is also as expected for our specific dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final remarks on the tradeoff between Recall and Precision: \n",
    "\n",
    "- Recall's formula: TP / (TP + FN), where FN represents the number of false negatives. \n",
    "\n",
    "- The recall is lower in the case of the model which includes SMOTE (compared to the case that we omit SMOTE from the pipeline).\n",
    "\n",
    "- Recall of the model with SMOTE = 21256 / ( 21256 + 2170) = 0.907\n",
    "\n",
    "- Recall of the model that omits SMOTE = 21910 / (21910 + 1516) = 0.935\n",
    "\n",
    "- These results are as expected. Indeed, the model without SMOTE \"sees\" fewer 0s (zeros) than the model which includes SMOTE, and thus the number of FN (false negatives) is much lower in the case that we omit SMOTE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
